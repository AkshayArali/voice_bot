<!doctype html>
<html>

<head>
  <meta charset="utf-8" />
  <title>Voice-Controlled Human-Following Robot</title>
  <style>
    @import url('https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;500;600&display=swap');

    :root {
      --bg: #0d1117;
      --panel: #111827;
      --card: #0f172a;
      --accent: #10b981;
      --accent-2: #3b82f6;
      --text: #e5e7eb;
      --muted: #9ca3af;
      --border: #1f2937;
      --glow: 0 20px 60px rgba(16, 185, 129, 0.2);
    }

    * {
      box-sizing: border-box;
    }

    body {
      font-family: 'Space Grotesk', -apple-system, BlinkMacSystemFont, 'Helvetica Neue', Arial, sans-serif;
      font-size: 12pt;
      line-height: 1.55;
      color: var(--text);
      background: radial-gradient(120% 120% at 10% 10%, rgba(16, 185, 129, 0.08), transparent 35%),
        radial-gradient(120% 120% at 90% 20%, rgba(59, 130, 246, 0.08), transparent 35%),
        var(--bg);
      margin: 0;
      padding: 0 12px 40px;
    }

    a {
      color: var(--accent-2);
    }

    h1,
    h2,
    h3,
    h4,
    h5,
    h6 {
      margin-top: 16px;
      letter-spacing: -0.01em;
    }

    pre {
      background: #0b1220;
      padding: 12px;
      border: 1px solid var(--border);
      white-space: pre-wrap;
      border-radius: 10px;
      box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.03);
      position: relative;
      font-size: 12px;
      line-height: 1.25;
    }

    pre[data-lang]::after {
      content: attr(data-lang);
      position: absolute;
      top: 8px;
      right: 10px;
      background: #0b1220;
      border: 1px solid var(--border);
      padding: 2px 6px;
      border-radius: 8px;
      font-size: 11px;
      color: #9ca3af;
    }

    pre.diagram {
      font-size: 9px;
      line-height: 1.05;
      padding: 8px;
      transform: scale(0.92);
      transform-origin: top left;
      display: inline-block;
      max-width: 100%;
    }

    code {
      font-family: Menlo, Consolas, 'Courier New', monospace;
    }

    /* D2 Diagram Styling */
    .d2-diagram {
      max-width: 100%;
      height: auto;
      max-height: 60vh;
      /* Prevent taking up too much vertical space */
      display: block;
      margin: 0 auto;
      border-radius: 8px;
      /* Add a subtle background to transparent SVGs if needed */
      background: rgba(255, 255, 255, 0.02);
    }

    table {
      border-collapse: collapse;
      width: 100%;
      margin: 10px 0;
      background: #0b1220;
      border-radius: 12px;
      overflow: hidden;
      border: 1px solid var(--border);
    }

    th,
    td {
      border: 1px solid var(--border);
      padding: 10px 12px;
      vertical-align: top;
    }

    th {
      background: #111a2e;
      color: #d1d5db;
      text-align: left;
    }

    tr:nth-child(even) td {
      background: rgba(255, 255, 255, 0.02);
    }

    hr {
      border: none;
      border-top: 1px solid var(--border);
      margin: 18px 0;
    }

    blockquote {
      border-left: 3px solid var(--accent);
      margin: 10px 0;
      padding-left: 12px;
      color: #cbd5e1;
    }

    .page {
      max-width: 1080px;
      margin: 0 auto;
    }

    .hero {
      background: linear-gradient(135deg, rgba(16, 185, 129, 0.16), rgba(59, 130, 246, 0.14));
      border: 1px solid var(--border);
      border-radius: 18px;
      padding: 20px 22px;
      margin: 22px 0;
      box-shadow: var(--glow);
    }

    .hero h1 {
      margin: 0 0 6px;
      font-size: 28px;
    }

    .hero p {
      margin: 4px 0;
      color: var(--muted);
    }

    .tag {
      display: inline-flex;
      align-items: center;
      gap: 8px;
      padding: 6px 10px;
      border-radius: 999px;
      background: rgba(15, 23, 42, 0.6);
      border: 1px solid var(--border);
      color: #d1fae5;
      font-weight: 600;
    }

    .pill-nav {
      position: sticky;
      top: 0;
      z-index: 10;
      backdrop-filter: blur(8px);
      background: rgba(13, 17, 23, 0.75);
      padding: 10px 0;
      margin: 12px 0 18px;
      display: flex;
      gap: 10px;
      flex-wrap: wrap;
    }

    .pill-nav a,
    .pill-nav button {
      border: 1px solid var(--border);
      color: var(--text);
      padding: 8px 12px;
      border-radius: 999px;
      background: #0b1220;
      text-decoration: none;
      font-weight: 600;
      transition: transform 150ms ease, box-shadow 150ms ease, border-color 150ms ease;
      cursor: pointer;
    }

    .pill-nav a:hover,
    .pill-nav button:hover {
      transform: translateY(-2px);
      border-color: var(--accent);
      box-shadow: 0 12px 30px rgba(16, 185, 129, 0.25);
    }

    .grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(260px, 1fr));
      gap: 14px;
      margin-bottom: 18px;
    }

    .card {
      background: var(--card);
      border: 1px solid var(--border);
      border-radius: 14px;
      padding: 14px;
      box-shadow: 0 10px 20px rgba(0, 0, 0, 0.25);
    }

    .card h3 {
      margin-top: 0;
    }

    .sparkline {
      display: flex;
      gap: 8px;
      align-items: center;
      font-weight: 600;
    }

    .sparkline-bar {
      height: 10px;
      border-radius: 999px;
      background: linear-gradient(90deg, var(--accent), var(--accent-2));
      flex: 1;
    }

    .small-label {
      color: var(--muted);
      font-size: 12px;
    }

    .meter {
      background: #0b1220;
      border-radius: 12px;
      border: 1px solid var(--border);
      overflow: hidden;
      margin-top: 8px;
      height: 12px;
    }

    .meter span {
      display: block;
      height: 100%;
      background: linear-gradient(90deg, var(--accent), var(--accent-2));
    }

    .chart-shell {
      position: relative;
      height: 240px;
    }

    .badge {
      display: inline-block;
      padding: 4px 8px;
      border-radius: 10px;
      background: rgba(16, 185, 129, 0.12);
      color: #a7f3d0;
      border: 1px solid var(--border);
      font-weight: 600;
    }
    .d2-diagram {
      display: block;
      max-width: 100%;
      height: auto;
      max-height: 520px;
      margin: 4px auto;
      object-fit: contain;
    }

    details summary {
      cursor: pointer;
      padding: 10px 12px;
      background: var(--card);
      border: 1px solid var(--border);
      border-radius: 12px;
      margin-bottom: 12px;
      list-style: none;
      font-weight: 700;
    }

    details[open] summary {
      border-color: var(--accent);
    }

    @media (max-width: 720px) {
      .hero {
        grid-template-columns: 1fr;
      }

      .pill-nav {
        position: static;
      }
    }
  </style>
  <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
</head>

<body>
  <div class="page">
    <div class="hero">
      <div>
        <h1 style="margin-top: 0;">Voice-Controlled Human-Following Robot</h1>
        <img src="Voice_bot_edit.jpg" alt="Voice Bot Robot" style="max-width: 100%; height: auto; max-height: 320px; border-radius: 14px; margin: 16px 0; box-shadow: 0 12px 40px rgba(0,0,0,0.4);" />
        <p style="font-size: 15px; font-style: italic; color: #a7f3d0; margin: 12px 0;">"Your voice is the controller — the robot that listens, sees, and follows."</p>
        <p style="color: var(--muted); margin-bottom: 8px;">AI-powered Create3 robot on Raspberry Pi 5 with edge inference</p>
        <p class="small-label">Team: Akshay Aralikatti, Himanashu Singh, Nicholas Panaccione</p>
        <a href="https://github.com/AkshayArali/voice_bot" target="_blank" rel="noopener noreferrer" style="display: inline-flex; align-items: center; gap: 6px; margin-top: 12px; padding: 8px 14px; background: rgba(15, 23, 42, 0.6); border: 1px solid var(--border); border-radius: 999px; color: var(--text); text-decoration: none; font-weight: 600; transition: border-color 150ms ease, box-shadow 150ms ease;">
          <svg height="18" width="18" viewBox="0 0 16 16" fill="currentColor"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"/></svg>
          Source Code
        </a>
      </div>
    </div>

    <nav class="pill-nav">
    <a href="#overview">Overview</a>
    <a href="#system-architecture">Architecture</a>
    <a href="#feature-deep-dives">Features</a>
    <a href="#requirements-metrics">Metrics</a>
    <a href="#performance-analysis">Performance</a>
    <a href="#demo-videos">Demo Videos</a>
    <a href="#installation">Install</a>
    <a href="#usage">Usage</a>
    <button id="toggle-details" type="button">Toggle detailed report</button>
    </nav>

    <div class="grid">
      <div class="card">
        <h3>Delivery Snapshot</h3>
        <p><strong>Edge AI:</strong> Wake word, ASR, intent parsing run locally. Vision uses Gemini API for scene
          description.</p>
        <p><strong>Control Loop:</strong> MediaPipe + ROS 2 Jazzy streaming to Create3 <code>/cmd_vel</code>.</p>
      </div>
      <div class="card chart-shell">
        <h3>Latency Per Module (ms)</h3>
        <canvas id="latencyChart" aria-label="Latency per module chart"></canvas>
      </div>
      <div class="card">
        <h3>Demo Flow</h3>
        <ul>
          <li>Say wake word “computer”.</li>
          <li>Speak command: “follow me” or “describe scene”.</li>
          <li>Pi routes intent → ROS 2 node → Create3 motion.</li>
          <li>Gemini vision replies with voice via <code>espeak-ng</code>.</li>
        </ul>
      </div>
    </div>

    <div id="demo-videos" class="card">
      <h3>Demo Videos</h3>
      <p class="small-label">Embedding is restricted on these links. Click “Open on YouTube” to view.</p>
      <div class="grid" style="grid-template-columns: repeat(auto-fit, minmax(260px, 1fr));">
        <div class="card">
          <h4>Turn right</h4>
          <a class="pill-nav" style="display:inline-flex; padding:8px 10px;" href="https://youtu.be/ci16BRO9Rsk" target="_blank" rel="noopener noreferrer">Open on YouTube</a>
        </div>
        <div class="card">
          <h4>Move forward</h4>
          <a class="pill-nav" style="display:inline-flex; padding:8px 10px;" href="https://youtu.be/Sx8OgqhmqBw" target="_blank" rel="noopener noreferrer">Open on YouTube</a>
        </div>
        <div class="card">
          <h4>Follow me</h4>
          <a class="pill-nav" style="display:inline-flex; padding:8px 10px;" href="https://youtu.be/FZl0FrQA05Q" target="_blank" rel="noopener noreferrer">Open on YouTube</a>
        </div>
        <div class="card">
          <h4>Describe surroundings</h4>
          <a class="pill-nav" style="display:inline-flex; padding:8px 10px;" href="https://youtu.be/oHm0RvT3wdA" target="_blank" rel="noopener noreferrer">Open on YouTube</a>
        </div>
        <div class="card">
          <h4>Chained commands</h4>
          <a class="pill-nav" style="display:inline-flex; padding:8px 10px;" href="https://youtu.be/-yZhTapCbVI" target="_blank" rel="noopener noreferrer">Open on YouTube</a>
        </div>
      </div>
    </div>

    <details open id="full-report">
      <summary>Full project write-up (toggle to collapse/expand)</summary>
      <div id="detailed-report">
        <p><strong>Date:</strong> 2025-12-17</p>
        <hr />
        <h2 id="table-of-contents">Table of Contents</h2>
        <ol>
          <li><a href="#overview">Overview</a>
          </li>
          <li><a href="#features--objectives">Features &amp; Objectives</a>
          </li>
          <li><a href="#system-architecture">System Architecture</a>
          </li>
          <li><a href="#tech-stack">Tech Stack</a>
          </li>
          <li><a href="#hardware-requirements">Hardware Requirements</a>
          </li>
          <li><a href="#feature-deep-dives">Feature Deep Dives</a>
          </li>
        </ol>
        <ul>
          <li><a href="#1-wake-word-detection-picovoice-porcupine">Wake Word Detection</a>
          </li>
          <li><a href="#2-speech-recognition-openai-whisper">Speech Recognition</a>
          </li>
          <li><a href="#3-intent-parsing-google-gemma">Intent Parsing</a>
          </li>
          <li><a href="#4-vision--scene-description-google-gemini">Vision &amp; Scene Description</a>
          </li>
          <li><a href="#5-human-following-mediapipe--opencv">Human Following</a>
          </li>
          <li><a href="#6-robot-control-ros-2">Robot Control</a>
          </li>
        </ul>
        <ol>
          <li><a href="#system-flow">System Flow</a>
          </li>
          <li><a href="#requirements-metrics">Requirements &amp; Metrics</a>
          </li>
          <li><a href="#performance-analysis">Performance Analysis</a>
          </li>
          <li><a href="#installation">Installation</a>
          </li>
          <li><a href="#usage">Usage</a>
          </li>
          <li><a href="#troubleshooting">Troubleshooting</a>
          </li>
        </ol>
        <hr />
        <h2 id="overview">Overview</h2>
        <p>This project transforms an iRobot Create3 into an intelligent voice-controlled robot capable of:</p>
        <ul>
          <li>Responding to wake word &quot;computer&quot;
          </li>
          <li>Understanding natural language commands
          </li>
          <li>Following humans using computer vision
          </li>
          <li>Describing its surroundings using AI vision
          </li>
          <li>Executing movement commands
          </li>
        </ul>
        <div class="card">
          <h3>Robot AI System Overview</h3>
          <img src="diagrams/system_overview.svg" class="d2-diagram" alt="System Overview" />
        </div>
        <hr />
        <h2 id="features-objectives">Features &amp; Objectives</h2>
        <h3 id="goals">Goals</h3>
        <table>
          <thead>
            <tr>
              <th>Goal</th>
              <th>Description</th>
              <th>Status</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>Voice Activation</strong></td>
              <td>Hands-free wake word detection</td>
              <td>Complete</td>
            </tr>
            <tr>
              <td><strong>Natural Language</strong></td>
              <td>Understand conversational commands</td>
              <td>Complete</td>
            </tr>
            <tr>
              <td><strong>Human Following</strong></td>
              <td>Autonomously follow a person</td>
              <td>Complete</td>
            </tr>
            <tr>
              <td><strong>Scene Understanding</strong></td>
              <td>Describe environment using AI</td>
              <td>Complete</td>
            </tr>
            <tr>
              <td><strong>Movement Control</strong></td>
              <td>Execute precise movements</td>
              <td>Complete</td>
            </tr>
            <tr>
              <td><strong>Low Latency</strong></td>
              <td>Responsive real-time control</td>
              <td>Optimized</td>
            </tr>
            <tr>
              <td><strong>Edge Deployment</strong></td>
              <td>Run entirely on Raspberry Pi 5</td>
              <td>Complete</td>
            </tr>
          </tbody>
        </table>
        <h3 id="objectives">Objectives</h3>
        <ol>
          <li><strong>Accessibility</strong>: Control robot without physical interface
          </li>
          <li><strong>Intelligence</strong>: Understand intent, not just keywords
          </li>
          <li><strong>Autonomy</strong>: Follow humans without manual control
          </li>
          <li><strong>Awareness</strong>: Perceive and describe environment
          </li>
          <li><strong>Efficiency</strong>: Run on resource-constrained hardware
          </li>
        </ol>
        <hr />
        <h2 id="system-architecture">System Architecture</h2>
        <h3 id="high-level-architecture">High-Level Architecture</h3>
        <div class="card">
          <img src="diagrams/high_level_arch.svg" class="d2-diagram" alt="High Level Architecture" />
        </div>
        <h3 id="component-interaction">Component Interaction</h3>
        <div class="card">
          <h3>Component Interaction Map</h3>
          <img src="diagrams/component_interaction.svg" class="d2-diagram" alt="Component Interaction" />
        </div>
        <hr />
        <h2 id="tech-stack">Tech Stack</h2>
        <table>
          <thead>
            <tr>
              <th>Layer</th>
              <th>Technology</th>
              <th>Purpose</th>
              <th>Model/Version</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>Wake Word</strong></td>
              <td>Picovoice Porcupine</td>
              <td>Detect &quot;computer&quot;</td>
              <td>Built-in keyword</td>
            </tr>
            <tr>
              <td><strong>Speech-to-Text</strong></td>
              <td>OpenAI Whisper</td>
              <td>Transcribe speech</td>
              <td><code>tiny</code> (39M params)</td>
            </tr>
            <tr>
              <td><strong>Intent Parsing</strong></td>
              <td>Google Gemma</td>
              <td>Understand commands</td>
              <td><code>gemma-3-1b-it</code> (Local)</td>
            </tr>
            <tr>
              <td><strong>Vision AI</strong></td>
              <td>Google Gemini</td>
              <td>Scene description</td>
              <td><code>gemini-2.5-flash</code> (API)</td>
            </tr>
            <tr>
              <td><strong>Person Detection</strong></td>
              <td>MediaPipe Pose</td>
              <td>Detect humans</td>
              <td>Pose Lite</td>
            </tr>
            <tr>
              <td><strong>Fallback Detection</strong></td>
              <td>OpenCV HOG</td>
              <td>Partial body detection</td>
              <td>Default SVM</td>
            </tr>
            <tr>
              <td><strong>Robot Middleware</strong></td>
              <td>ROS 2 Jazzy</td>
              <td>Robot communication</td>
              <td>Jazzy Jalisco</td>
            </tr>
            <tr>
              <td><strong>Text-to-Speech</strong></td>
              <td>espeak-ng</td>
              <td>Voice feedback</td>
              <td>System TTS</td>
            </tr>
            <tr>
              <td><strong>OS</strong></td>
              <td>Ubuntu 24.04</td>
              <td>Operating system</td>
              <td>ARM64</td>
            </tr>
          </tbody>
        </table>
        <h3 id="model-comparison">Model Comparison</h3>
        <h3 id="model-comparison">Model Comparison</h3>
        <div class="card">
          <table>
            <thead>
              <tr>
                <th>Model</th>
                <th>Size</th>
                <th>Runs On</th>
                <th>Latency</th>
                <th>Purpose</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><strong>Porcupine</strong></td>
                <td><span class="badge">~2 MB</span></td>
                <td>Local CPU</td>
                <td><span class="tag" style="background:#dcfce7; color:#166534">&lt;10ms</span></td>
                <td>Wake word</td>
              </tr>
              <tr>
                <td><strong>Whisper tiny</strong></td>
                <td><span class="badge">39 MB</span></td>
                <td>Local CPU</td>
                <td><span class="tag" style="background:#fee2e2; color:#991b1b">~2-3s</span></td>
                <td>ASR</td>
              </tr>
              <tr>
                <td><strong>Gemma 3 1B</strong></td>
                <td><span class="badge">~2 GB</span></td>
                <td>Local CPU</td>
                <td><span class="tag" style="background:#ffedd5; color:#9a3412">~1-2s</span></td>
                <td>Intent</td>
              </tr>
              <tr>
                <td><strong>Gemini Flash</strong></td>
                <td>Large</td>
                <td>Cloud API</td>
                <td><span class="tag" style="background:#ffedd5; color:#9a3412">~2-3s</span></td>
                <td>Vision</td>
              </tr>
              <tr>
                <td><strong>MediaPipe</strong></td>
                <td><span class="badge">~5 MB</span></td>
                <td>Local CPU</td>
                <td><span class="tag" style="background:#dcfce7; color:#166534">~50ms</span></td>
                <td>Detection</td>
              </tr>
            </tbody>
          </table>
        </div>
        <hr />
        <h2 id="hardware-requirements">Hardware Requirements</h2>
        <h3 id="required-components">Required Components</h3>
        <table>
          <thead>
            <tr>
              <th>Component</th>
              <th>Specification</th>
              <th>Purpose</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>iRobot Create3</strong></td>
              <td>Educational robot</td>
              <td>Mobile base</td>
            </tr>
            <tr>
              <td><strong>Raspberry Pi 5</strong></td>
              <td>16GB RAM</td>
              <td>Compute unit</td>
            </tr>
            <tr>
              <td><strong>USB Webcam</strong></td>
              <td>720p minimum</td>
              <td>Vision input</td>
            </tr>
            <tr>
              <td><strong>USB Microphone</strong></td>
              <td>Any USB mic</td>
              <td>Voice input</td>
            </tr>
            <tr>
              <td><strong>Speaker</strong></td>
              <td>Bluetooth or USB</td>
              <td>Voice output</td>
            </tr>
            <tr>
              <td><strong>Power</strong></td>
              <td>USB-C for Pi</td>
              <td>Power supply</td>
            </tr>
          </tbody>
        </table>
        <h3 id="connection-diagram">Connection Diagram</h3>
        <div class="card">
          <img src="diagrams/hardware_connections.svg" class="d2-diagram" alt="Hardware Connections" />
        </div>
        <hr />
        <h2 id="feature-deep-dives">Feature Deep Dives</h2>
        <h3 id="1-wake-word-detection-picovoice-porcupine">1. Wake Word Detection (Picovoice Porcupine)</h3>
        <p><strong>Purpose</strong>: Always-on listening for &quot;computer&quot; trigger word without cloud
          connectivity.</p>
        <div class="card">
          <h3>Wake Word Detection Flow</h3>
          <img src="diagrams/wake_word_flow.svg" class="d2-diagram" alt="Wake Word Flow" />
        </div>
        <p><strong>Key Specifications</strong>:</p>
        <ul>
          <li><strong>Latency</strong>: &lt;10ms detection
          </li>
          <li><strong>CPU Usage</strong>: ~5% (always listening)
          </li>
          <li><strong>Memory</strong>: ~2MB model
          </li>
          <li><strong>Accuracy</strong>: &gt;95% at 1m distance
          </li>
          <li><strong>False Positive Rate</strong>: &lt;1 per 10 hours
          </li>
        </ul>
        <p><strong>Code Implementation</strong> (<code>main.py</code>):</p>
        <pre><code class="language-python">import pvporcupine
from pvrecorder import PvRecorder

# Initialize Porcupine wake word engine
porcupine = pvporcupine.create(
    access_key=PICOVOICE_ACCESS_KEY,
    keywords=[&quot;computer&quot;],
)

# Create audio recorder
recorder = PvRecorder(
    device_index=device_index,
    frame_length=porcupine.frame_length,
)

# Wake word detection loop
recorder.start()
while rclpy.ok():
    pcm = recorder.read()
    result = porcupine.process(pcm)
    if result &gt;= 0:
        print(&quot;[WAKE] &#x27;Computer&#x27; detected!&quot;)
        break</code></pre>
        <hr />
        <h3 id="2-speech-recognition-openai-whisper">2. Speech Recognition (OpenAI Whisper)</h3>
        <p><strong>Purpose</strong>: Convert spoken commands to text after wake word detection.</p>
        <div class="card">
          <h3>Speech Recognition Flow</h3>
          <img src="diagrams/speech_rec_flow.svg" class="d2-diagram" alt="Speech Recognition Flow" />
        </div>
        <p><strong>Key Specifications</strong>:</p>
        <ul>
          <li><strong>Model</strong>: Whisper tiny (39M parameters)
          </li>
          <li><strong>Latency</strong>: 2-3 seconds for 6s audio
          </li>
          <li><strong>Accuracy</strong>: ~85% WER on conversational speech
          </li>
          <li><strong>Memory</strong>: ~150MB loaded
          </li>
          <li><strong>Runs</strong>: Locally on CPU (no GPU required)
          </li>
        </ul>
        <p><strong>Code Implementation</strong> (<code>main.py</code>):</p>
        <pre><code class="language-python">import whisper

class WhisperRecognizer:
    &quot;&quot;&quot;Record audio with arecord and transcribe with Whisper.&quot;&quot;&quot;

    def __init__(self, model_name: str = &quot;tiny&quot;, sample_rate: int = 16000):
        self.model_name = model_name
        self.sample_rate = sample_rate
        print(f&quot;[ASR] Loading Whisper model &#x27;{model_name}&#x27;...&quot;)
        self.model = whisper.load_model(model_name)
        print(&quot;[ASR] Whisper model loaded.&quot;)

    def listen_and_transcribe(self, max_seconds: float = 6.0) -&gt; str:
        &quot;&quot;&quot;Record audio and transcribe.&quot;&quot;&quot;
        cmd = [
            &quot;arecord&quot;, &quot;-D&quot;, ARECORD_DEVICE,
            &quot;-f&quot;, &quot;S16_LE&quot;, &quot;-r&quot;, str(self.sample_rate),
            &quot;-c&quot;, &quot;1&quot;, &quot;-d&quot;, str(int(max_seconds)),
            &quot;-t&quot;, &quot;wav&quot;, str(LAST_COMMAND_WAV),
        ]
        subprocess.run(cmd, check=True, capture_output=True)

        result = self.model.transcribe(str(LAST_COMMAND_WAV), fp16=False)
        text = (result.get(&quot;text&quot;) or &quot;&quot;).strip()
        return text</code></pre>
        <hr />
        <h3 id="3-intent-parsing-google-gemma">3. Intent Parsing (Google Gemma)</h3>
        <p><strong>Purpose</strong>: Convert natural language text into structured robot commands using local LLM
          inference.</p>
        <div class="card">
          <h3>Intent Parsing Flow</h3>
          <img src="diagrams/intent_parsing_flow.svg" class="d2-diagram" alt="Intent Parsing Flow" />
        </div>
        <p><strong>Code Implementation</strong> (<code>robot/intent.py</code>):</p>
        <pre><code class="language-python">import google.generativeai as genai

# Gemma model for local intent parsing
GEMMA_MODEL_NAME = os.environ.get(&quot;GEMMA_MODEL_NAME&quot;, &quot;gemma-3-1b-it&quot;)

SYSTEM_INSTRUCTIONS = &quot;&quot;&quot;
You are an intent parser for a small mobile robot.

Your job is to convert a short spoken command into a JSON object that
describes what the robot should do.

SUPPORTED INTENTS:

1) &quot;movement_sequence&quot;
- User wants the robot to move and/or turn.
- Return:
    {
    &quot;intent&quot;: &quot;movement_sequence&quot;,
    &quot;actions&quot;: [
        {&quot;type&quot;: &quot;move&quot;, &quot;direction&quot;: &quot;forward&quot;, &quot;distance_m&quot;: 1.0},
        {&quot;type&quot;: &quot;turn&quot;, &quot;direction&quot;: &quot;left&quot;, &quot;degrees&quot;: 90}
    ]
    }

2) &quot;follow_me&quot; - User wants robot to follow them
3) &quot;describe_surroundings&quot; - User wants robot to describe what it sees
4) &quot;stop&quot; - User wants robot to stop immediately
5) &quot;shutdown&quot; - User wants to exit program

ALWAYS respond with PURE JSON. Do NOT include explanations.
&quot;&quot;&quot;

def parse_intent(text: str) -&gt; Dict[str, Any]:
    &quot;&quot;&quot;Parse user command into structured intent using Gemma.&quot;&quot;&quot;
    genai.configure(api_key=API_KEY)
    model = genai.GenerativeModel(GEMMA_MODEL_NAME)
    
    prompt = SYSTEM_INSTRUCTIONS + &quot;\n\nUser command:\n&quot; + text + &quot;\n\nJSON:&quot;
    response = model.generate_content(prompt)
    
    json_str = _extract_json(response.text)
    return json.loads(json_str)</code></pre>
        <p><strong>Fallback Parser</strong>: If API fails, keyword-based parsing is used:</p>
        <pre><code class="language-python">def _fallback_keyword_parser(text: str) -&gt; Dict[str, Any]:
    &quot;&quot;&quot;Simple backup parser if Gemma is unavailable.&quot;&quot;&quot;
    lower = text.lower()

    if any(w in lower for w in [&quot;stop&quot;, &quot;freeze&quot;]):
        return {&quot;intent&quot;: &quot;stop&quot;, &quot;actions&quot;: [], &quot;raw&quot;: text}

    if any(w in lower for w in [&quot;follow me&quot;, &quot;follow&quot;, &quot;track me&quot;]):
        return {&quot;intent&quot;: &quot;follow_me&quot;, &quot;actions&quot;: [], &quot;raw&quot;: text}

    if &quot;forward&quot; in lower:
        return {
            &quot;intent&quot;: &quot;movement_sequence&quot;,
            &quot;actions&quot;: [{&quot;type&quot;: &quot;move&quot;, &quot;direction&quot;: &quot;forward&quot;, &quot;distance_m&quot;: 0.3}],
            &quot;raw&quot;: text,
        }

    return {&quot;intent&quot;: &quot;unknown&quot;, &quot;actions&quot;: [], &quot;raw&quot;: text}</code></pre>
        <hr />
        <h3 id="4-vision-scene-description-google-gemini">4. Vision &amp; Scene Description (Google Gemini)</h3>
        <p><strong>Purpose</strong>: Capture image and generate natural language description of surroundings.</p>
        <div class="card">
          <h3>Vision Description Flow</h3>
          <img src="diagrams/vision_flow.svg" class="d2-diagram" alt="Vision Description Flow" />
        </div>
        <p><strong>Code Implementation</strong> (<code>robot/vision.py</code>):</p>
        <pre><code class="language-python">import cv2
import google.generativeai as genai

GEMINI_MODEL = os.getenv(&quot;GEMINI_VISION_MODEL&quot;, &quot;gemini-2.5-flash&quot;)

def capture_frame(device_index: int = 0, warmup_frames: int = 5) -&gt; Path:
    &quot;&quot;&quot;Capture a single frame from camera and save to IMAGE_PATH.&quot;&quot;&quot;
    cap = cv2.VideoCapture(device_index, cv2.CAP_V4L2)
    if not cap.isOpened():
        cap = cv2.VideoCapture(device_index)

    # Warm-up frames so exposure/auto-settings settle
    for _ in range(warmup_frames):
        cap.read()

    ret, frame = cap.read()
    cap.release()

    cv2.imwrite(str(IMAGE_PATH), frame)
    return IMAGE_PATH


def describe_scene_with_gemini(device_index: int = 0) -&gt; str:
    &quot;&quot;&quot;Capture an image and ask Gemini to describe it.&quot;&quot;&quot;
    genai.configure(api_key=os.getenv(&quot;GEMINI_API_KEY&quot;))

    prompt = (
        &quot;You are a mobile robot with a forward-facing camera. &quot;
        &quot;Describe what you see in front of you as if you are talking to a human user. &quot;
        &quot;Mention important objects, distances in rough terms (like &#x27;near&#x27;, &#x27;far&#x27;, &quot;
        &quot;&#x27;to your left/right&#x27;), and anything that might affect navigation.&quot;
    )

    img_path = capture_frame(device_index=device_index)
    with open(img_path, &quot;rb&quot;) as f:
        img_bytes = f.read()

    model = genai.GenerativeModel(GEMINI_MODEL)
    response = model.generate_content([
        prompt,
        {&quot;mime_type&quot;: &quot;image/jpeg&quot;, &quot;data&quot;: img_bytes},
    ])

    return response.text.strip()</code></pre>
        <hr />
        <h3 id="5-human-following-mediapipe-opencv">5. Human Following (MediaPipe + OpenCV)</h3>
        <p><strong>Purpose</strong>: Detect and follow a human using computer vision.</p>
        <div class="card">
          <h3>Human Following System</h3>
          <img src="diagrams/human_following.svg" class="d2-diagram" alt="Human Following System" />
        </div>
        <p><strong>Detection Hybrid Logic</strong>:</p>
        <div class="card">
          <h3>Detection Hybrid Logic</h3>
          <img src="diagrams/hybrid_logic.svg" class="d2-diagram" alt="Detection Hybrid Logic" />
        </div>
        <p><strong>Code Implementation</strong> (<code>follow_hsr/detector.py</code>):</p>
        <pre><code class="language-python">import cv2
import mediapipe as mp
from dataclasses import dataclass

@dataclass
class PersonDetection:
    &quot;&quot;&quot;Represents a detected person in the frame.&quot;&quot;&quot;
    bbox: Tuple[int, int, int, int]  # (x, y, width, height)
    center_x: int
    center_y: int
    area: int
    confidence: float
    normalized_x: float    # X position normalized to [-1, 1]
    normalized_area: float # Area normalized relative to frame size


class PersonDetector:
    &quot;&quot;&quot;
    Detects people using MediaPipe + HOG hybrid for robust detection.
    &quot;&quot;&quot;

    def __init__(self, use_mediapipe: bool = True):
        # MediaPipe for full body detection
        self.mp_pose = mp.solutions.pose
        self.pose = self.mp_pose.Pose(
            static_image_mode=False,
            model_complexity=0,  # 0=lite for speed
            min_detection_confidence=0.3,
            min_tracking_confidence=0.3,
        )
        
        # HOG fallback for partial body (legs only)
        self.hog = cv2.HOGDescriptor()
        self.hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())

    def detect(self, frame: np.ndarray) -&gt; Optional[PersonDetection]:
        &quot;&quot;&quot;Detect person - MediaPipe first, HOG fallback.&quot;&quot;&quot;
        detection = self._detect_mediapipe(frame)
        
        if detection is None:
            detection = self._detect_hog(frame)
            
        return detection

    def _detect_mediapipe(self, frame: np.ndarray) -&gt; Optional[PersonDetection]:
        &quot;&quot;&quot;Detect person using MediaPipe Pose landmarks.&quot;&quot;&quot;
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.pose.process(rgb_frame)

        if not results.pose_landmarks:
            return None

        landmarks = results.pose_landmarks.landmark
        
        # Get visible landmark coordinates
        x_coords = [lm.x * self.frame_width for lm in landmarks if lm.visibility &gt; 0.2]
        y_coords = [lm.y * self.frame_height for lm in landmarks if lm.visibility &gt; 0.2]

        if len(x_coords) &lt; 3:
            return None

        # Calculate bounding box
        x_min, x_max = int(min(x_coords)), int(max(x_coords))
        y_min, y_max = int(min(y_coords)), int(max(y_coords))
        
        center_x = (x_min + x_max) // 2
        normalized_x = (center_x - self.frame_width / 2) / (self.frame_width / 2)

        return PersonDetection(...)</code></pre>
        <p><strong>Code Implementation</strong> (<code>follow_hsr/controller.py</code>):</p>
        <pre><code class="language-python">@dataclass
class ControllerConfig:
    &quot;&quot;&quot;Configuration for the follow controller.&quot;&quot;&quot;
    angular_kp: float = 0.8          # Proportional gain for turning
    max_angular_speed: float = 1.0   # rad/s
    angular_deadzone: float = 0.1    # Don&#x27;t turn if person nearly centered
    max_linear_speed: float = 0.2    # m/s


class FollowController:
    &quot;&quot;&quot;Simple proportional controller for following a person.&quot;&quot;&quot;
    
    def compute(self, detection: PersonDetection, dt: float) -&gt; Tuple[float, float]:
        &quot;&quot;&quot;Compute velocity commands based on person detection.&quot;&quot;&quot;
        if detection is None:
            return (0.0, 0.0)
        
        # Angular velocity - turn toward person
        x_error = detection.normalized_x
        if abs(x_error) &lt; self.config.angular_deadzone:
            angular_vel = 0.0
        else:
            angular_vel = -self.config.angular_kp * x_error
        
        # Linear velocity - faster when person is far, slower when close
        area = detection.normalized_area
        if area &gt; 0.4:
            linear_vel = self.config.max_linear_speed * 0.3  # Close
        elif area &gt; 0.2:
            linear_vel = self.config.max_linear_speed * 0.5  # Medium
        else:
            linear_vel = self.config.max_linear_speed        # Far
        
        return (linear_vel, angular_vel)</code></pre>
        <hr />
        <h3 id="6-robot-control-ros-2">6. Robot Control (ROS 2)</h3>
        <p><strong>Purpose</strong>: Interface with iRobot Create3 hardware via ROS 2 middleware.</p>
        <div class="card">
          <h3>ROS 2 Control Architecture</h3>
          <img src="diagrams/ros2_control.svg" class="d2-diagram" alt="ROS 2 Control Architecture" />
        </div>
        <p><strong>Code Implementation</strong> (<code>main.py</code>):</p>
        <pre><code class="language-python">import rclpy
from rclpy.node import Node
from geometry_msgs.msg import Twist

class RobotController(Node):
    &quot;&quot;&quot;ROS 2 node for controlling Create3.&quot;&quot;&quot;

    def __init__(self):
        super().__init__(&quot;robot_ai_controller&quot;)
        self.cmd_pub = self.create_publisher(Twist, &quot;/cmd_vel&quot;, 10)

    def stop(self):
        &quot;&quot;&quot;Stop all movement.&quot;&quot;&quot;
        twist = Twist()
        self.cmd_pub.publish(twist)

    def send_twist(self, linear_x: float, angular_z: float, duration: float):
        &quot;&quot;&quot;Send velocity for a duration.&quot;&quot;&quot;
        twist = Twist()
        twist.linear.x = float(linear_x)
        twist.angular.z = float(angular_z)

        end_time = time.time() + duration
        while time.time() &lt; end_time and rclpy.ok():
            self.cmd_pub.publish(twist)
            time.sleep(0.05)

        self.stop()

    def execute_movement(self, actions: List[Dict]):
        &quot;&quot;&quot;Execute a movement sequence from parsed intent.&quot;&quot;&quot;
        for action in actions:
            atype = action.get(&quot;type&quot;)

            if atype == &quot;move&quot;:
                direction = action.get(&quot;direction&quot;, &quot;forward&quot;)
                dist_m = float(action.get(&quot;distance_m&quot;, 0.3))
                duration = abs(dist_m) / MOVE_SPEED_M_S
                lin = MOVE_SPEED_M_S if direction == &quot;forward&quot; else -MOVE_SPEED_M_S
                self.send_twist(lin, 0.0, duration)

            elif atype == &quot;turn&quot;:
                direction = action.get(&quot;direction&quot;, &quot;right&quot;)
                degrees = float(action.get(&quot;degrees&quot;, 90.0))
                angle_rad = math.radians(abs(degrees))
                duration = angle_rad / TURN_SPEED_RAD_S
                ang = TURN_SPEED_RAD_S if direction == &quot;left&quot; else -TURN_SPEED_RAD_S
                self.send_twist(0.0, ang, duration)</code></pre>
        <hr />
        <h2 id="system-flow">System Flow</h2>
        <div class="card">
          <img src="diagrams/system_sequence.svg" class="d2-diagram" alt="System Flow Sequence" />
        </div>
        <h3 id="complete-command-flow">Complete Command Flow</h3>
        <h3 id="complete-command-flow">Complete Command Flow</h3>
        <div class="card">
          <img src="diagrams/command_flow.svg" class="d2-diagram" alt="Complete Command Flow" />
        </div>
      </div>
      <hr />
      <h2 id="requirements-metrics">Requirements &amp; Metrics</h2>
      <h3 id="functional-requirements">Functional Requirements</h3>
      <table>
        <thead>
          <tr>
            <th>ID</th>
            <th>Requirement</th>
            <th>Priority</th>
            <th>Status</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>FR1</td>
            <td>System shall detect wake word &quot;computer&quot;</td>
            <td>High</td>
            <td>Done</td>
          </tr>
          <tr>
            <td>FR2</td>
            <td>System shall transcribe speech to text</td>
            <td>High</td>
            <td>Done</td>
          </tr>
          <tr>
            <td>FR3</td>
            <td>System shall parse natural language to intents</td>
            <td>High</td>
            <td>Done</td>
          </tr>
          <tr>
            <td>FR4</td>
            <td>System shall execute movement commands</td>
            <td>High</td>
            <td>Done</td>
          </tr>
          <tr>
            <td>FR5</td>
            <td>System shall follow detected humans</td>
            <td>High</td>
            <td>Done</td>
          </tr>
          <tr>
            <td>FR6</td>
            <td>System shall describe surroundings on request</td>
            <td>Medium</td>
            <td>Done</td>
          </tr>
          <tr>
            <td>FR7</td>
            <td>System shall provide voice feedback</td>
            <td>Medium</td>
            <td>Done</td>
          </tr>
          <tr>
            <td>FR8</td>
            <td>System shall run on Raspberry Pi 5</td>
            <td>High</td>
            <td>Done</td>
          </tr>
        </tbody>
      </table>
      <h3 id="non-functional-requirements">Non-Functional Requirements</h3>
      <table>
        <thead>
          <tr>
            <th>ID</th>
            <th>Requirement</th>
            <th>Target</th>
            <th>Actual</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>NFR1</td>
            <td>Wake word detection latency</td>
            <td>&lt;100ms</td>
            <td>~10ms</td>
          </tr>
          <tr>
            <td>NFR2</td>
            <td>Speech recognition latency</td>
            <td>&lt;5s</td>
            <td>~3s</td>
          </tr>
          <tr>
            <td>NFR3</td>
            <td>Intent parsing latency</td>
            <td>&lt;3s</td>
            <td>~1-2s</td>
          </tr>
          <tr>
            <td>NFR4</td>
            <td>Following frame rate</td>
            <td>&gt;5 FPS</td>
            <td>~5 FPS</td>
          </tr>
          <tr>
            <td>NFR5</td>
            <td>CPU usage (following)</td>
            <td>&lt;50%</td>
            <td>~30%</td>
          </tr>
          <tr>
            <td>NFR6</td>
            <td>Memory usage</td>
            <td>&lt;2GB</td>
            <td>~1.5GB</td>
          </tr>
          <tr>
            <td>NFR7</td>
            <td>Wake word accuracy</td>
            <td>&gt;90%</td>
            <td>~95%</td>
          </tr>
        </tbody>
      </table>
      <h3 id="performance-metrics">Performance Metrics</h3>
      <div class="card">
        <h4>Latency Breakdown (Voice Command)</h4>
        <table>
          <tr>
            <td>Wake Word</td>
            <td>
              <div style="background:#e5e7eb; width:100%; height:8px; border-radius:4px;">
                <div style="background:#3b82f6; width:1%; height:8px; border-radius:4px;"></div>
              </div>
            </td>
            <td style="width:80px; text-align:right;">~10 ms</td>
          </tr>
          <tr>
            <td>Audio Rec</td>
            <td>
              <div style="background:#e5e7eb; width:100%; height:8px; border-radius:4px;">
                <div style="background:#3b82f6; width:60%; height:8px; border-radius:4px;"></div>
              </div>
            </td>
            <td style="width:80px; text-align:right;">6000 ms</td>
          </tr>
          <tr>
            <td>Whisper</td>
            <td>
              <div style="background:#e5e7eb; width:100%; height:8px; border-radius:4px;">
                <div style="background:#3b82f6; width:25%; height:8px; border-radius:4px;"></div>
              </div>
            </td>
            <td style="width:80px; text-align:right;">2500 ms</td>
          </tr>
          <tr>
            <td>Gemma</td>
            <td>
              <div style="background:#e5e7eb; width:100%; height:8px; border-radius:4px;">
                <div style="background:#3b82f6; width:15%; height:8px; border-radius:4px;"></div>
              </div>
            </td>
            <td style="width:80px; text-align:right;">1500 ms</td>
          </tr>
        </table>
        <p class="small-label">Total Worst Case: ~10.2s</p>

        <h4>Following Performance</h4>
        <table>
          <tr>
            <td>Total Latency per Frame</td>
            <td style="text-align:right;">~120 ms</td>
          </tr>
          <tr>
            <td>Actual Frame Rate</td>
            <td style="text-align:right;">~5 FPS</td>
          </tr>
        </table>
      </div>
      <hr />
      <h2 id="performance-analysis">Performance Analysis</h2>
      <h3 id="error-analysis">Error Analysis</h3>
      <table>
        <thead>
          <tr>
            <th>Component</th>
            <th>Error Type</th>
            <th>Rate</th>
            <th>Mitigation</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Wake Word</td>
            <td>False positive</td>
            <td>&lt;0.1/hr</td>
            <td>High threshold</td>
          </tr>
          <tr>
            <td>Wake Word</td>
            <td>False negative</td>
            <td>~5%</td>
            <td>Retry prompt</td>
          </tr>
          <tr>
            <td>Whisper</td>
            <td>Transcription error</td>
            <td>~15% WER</td>
            <td>Keyword fallback</td>
          </tr>
          <tr>
            <td>Gemma</td>
            <td>Intent misparse</td>
            <td>~10%</td>
            <td>Fallback parser</td>
          </tr>
          <tr>
            <td>MediaPipe</td>
            <td>Detection miss</td>
            <td>~20% close range</td>
            <td>HOG fallback</td>
          </tr>
          <tr>
            <td>HOG</td>
            <td>False positive</td>
            <td>~5%</td>
            <td>Area filtering</td>
          </tr>
        </tbody>
      </table>
      <h3 id="repeatability">Repeatability</h3>
      <table>
        <thead>
          <tr>
            <th>Action</th>
            <th>Repeatability</th>
            <th>Notes</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Wake word detection</td>
            <td>95%</td>
            <td>Consistent in quiet environments</td>
          </tr>
          <tr>
            <td>&quot;Move forward 1m&quot;</td>
            <td>+/-10cm</td>
            <td>Depends on floor surface</td>
          </tr>
          <tr>
            <td>&quot;Turn left 90deg&quot;</td>
            <td>+/-5deg</td>
            <td>Calibrated timing-based</td>
          </tr>
          <tr>
            <td>Person following</td>
            <td>80% tracking</td>
            <td>Loses track at edges</td>
          </tr>
        </tbody>
      </table>
      <h3 id="timing-analysis">Timing Analysis</h3>
      <div class="card">
        <img src="diagrams/timing_analysis.svg" class="d2-diagram" alt="Timing Analysis" />
      </div>
      <h3 id="efficiency-optimizations">Efficiency Optimizations</h3>
      <table>
        <thead>
          <tr>
            <th>Optimization</th>
            <th>Impact</th>
            <th>Implementation</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Low resolution (320x240)</td>
            <td>-75% pixels</td>
            <td>Camera settings</td>
          </tr>
          <tr>
            <td>Frame skipping (every 2nd)</td>
            <td>-50% detection calls</td>
            <td>Counter in loop</td>
          </tr>
          <tr>
            <td>Whisper tiny model</td>
            <td>-90% vs base</td>
            <td>Model selection</td>
          </tr>
          <tr>
            <td>Min sleep (50ms)</td>
            <td>CPU breathing room</td>
            <td>Sleep in loop</td>
          </tr>
          <tr>
            <td>HOG fallback</td>
            <td>Better close-range</td>
            <td>Hybrid detection</td>
          </tr>
        </tbody>
      </table>
      <hr />
      <h2 id="installation">Installation</h2>
      <h3 id="prerequisites">Prerequisites</h3>
      <pre><code class="language-bash"># Ubuntu 24.04 on Raspberry Pi 5
# ROS 2 Jazzy installed

# System dependencies
sudo apt update
sudo apt install -y python3-pip python3-venv \
    alsa-utils espeak-ng portaudio19-dev</code></pre>
      <h3 id="setup">Setup</h3>
      <pre><code class="language-bash"># Clone repository
git clone https://github.com/himanshusr/voice_follow_create3.git robot_ai
cd robot_ai

# Create virtual environment
python3 -m venv .venv
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Set environment variables
export GEMINI_API_KEY=&quot;your-gemini-key&quot;
export PICOVOICE_ACCESS_KEY=&quot;your-picovoice-key&quot;</code></pre>
      <h3 id="get-api-keys">Get API Keys</h3>
      <table>
        <thead>
          <tr>
            <th>Service</th>
            <th>URL</th>
            <th>Free Tier</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Gemini</td>
            <td>https://aistudio.google.com/app/apikey</td>
            <td>Yes</td>
          </tr>
          <tr>
            <td>Picovoice</td>
            <td>https://console.picovoice.ai/</td>
            <td>Yes (limited)</td>
          </tr>
        </tbody>
      </table>
      <hr />
      <h2 id="usage">Usage</h2>
      <h3 id="quick-start">Quick Start</h3>
      <pre><code class="language-bash">cd ~/robot_ai
source /opt/ros/jazzy/setup.bash
source .venv/bin/activate
export GEMINI_API_KEY=&quot;your-key&quot;
export PICOVOICE_ACCESS_KEY=&quot;your-key&quot;

python main.py</code></pre>
      <h3 id="voice-commands">Voice Commands</h3>
      <table>
        <thead>
          <tr>
            <th>Say This</th>
            <th>Robot Does</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>&quot;Computer&quot;</td>
            <td>Wakes up, says &quot;Yes?&quot;</td>
          </tr>
          <tr>
            <td>&quot;Follow me&quot;</td>
            <td>Starts following you</td>
          </tr>
          <tr>
            <td>&quot;Stop&quot;</td>
            <td>Stops all movement</td>
          </tr>
          <tr>
            <td>&quot;Go forward one meter&quot;</td>
            <td>Moves forward 1m</td>
          </tr>
          <tr>
            <td>&quot;Turn left ninety degrees&quot;</td>
            <td>Turns left 90 degrees</td>
          </tr>
          <tr>
            <td>&quot;What do you see?&quot;</td>
            <td>Describes surroundings</td>
          </tr>
          <tr>
            <td>&quot;Shut down&quot;</td>
            <td>Exits program</td>
          </tr>
        </tbody>
      </table>
      <hr />
      <h2 id="troubleshooting">Troubleshooting</h2>
      <h3 id="common-issues">Common Issues</h3>
      <table>
        <thead>
          <tr>
            <th>Issue</th>
            <th>Cause</th>
            <th>Solution</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>&quot;computer&quot; not detected</td>
            <td>Mic not working</td>
            <td>Check <code>arecord -l</code></td>
          </tr>
          <tr>
            <td>Robot doesn&#x27;t move</td>
            <td>Create3 not connected</td>
            <td>Check <code>ros2 topic list</code></td>
          </tr>
          <tr>
            <td>High CPU / crashes</td>
            <td>Too intensive</td>
            <td>Use <code>--no-video</code> flag</td>
          </tr>
          <tr>
            <td>No speech output</td>
            <td>Speaker not set</td>
            <td>Check <code>aplay -l</code> or <code>pactl list sinks</code></td>
          </tr>
          <tr>
            <td>Intent parsing fails</td>
            <td>No API key</td>
            <td>Set <code>GEMINI_API_KEY</code></td>
          </tr>
        </tbody>
      </table>
      <h3 id="debug-commands">Debug Commands</h3>
      <pre><code class="language-bash"># Check microphone
arecord -d 3 test.wav &amp;&amp; aplay test.wav

# Check camera
python -c &quot;import cv2; print(cv2.VideoCapture(0).isOpened())&quot;

# Check ROS connection
ros2 topic list | grep cmd_vel

# Check Create3 battery
ros2 topic echo /battery_state --once | grep percentage

# Check Bluetooth speaker
pactl list sinks short</code></pre>
      <hr />
      <h2 id="conclusion">Conclusion & Future Work</h2>
      <div class="card">
        <p>
          This lab successfully demonstrated a fully integrated Voice-Controlled AI Robot using the Raspberry Pi 5 and
          iRobot Create3. By combining <strong>on-device wake word detection</strong> (Porcupine), <strong>speech
            recognition</strong> (Whisper), and <strong>local LLM intent parsing</strong> (Gemma), we achieved a
          privacy-focused and responsive control system. The addition of <strong>cloud-based Vision</strong> (Gemini)
          added multimodal capabilities, allowing the robot to understand and describe its environment.
        </p>
        <p>
          Key achievements include:
        </p>
        <ul>
          <li><strong>Low Latency Control</strong>: ~1.5s latency for movement commands.</li>
          <li><strong>Robust Following</strong>: Hybrid MediaPipe + HOG tracking for reliable human following.</li>
          <li><strong>Natural Interaction</strong>: Conversational interface rather than rigid keyword matching.</li>
        </ul>
        <h4>Future Improvements</h4>
        <ul>
          <li><strong>SLAM Integration</strong>: Adding Nav2 for map-based navigation ("Go to the kitchen").</li>
          <li><strong>Faster Vision</strong>: optimizing the vision pipeline or using a local vision model (e.g.,
            PaliGemma).</li>
          <li><strong>Speaker Diarization</strong>: Distinguishing between multiple users.</li>
        </ul>
      </div>
      <hr />
      <h2 id="license">License</h2>
      <p>MIT License - See LICENSE file for details.</p>
      <hr />
      <h2 id="contributors">Contributors</h2>
      <ul>
        <li>Human-Following Module: MediaPipe + OpenCV hybrid detection
        </li>
        <li>Voice Control: Picovoice + Whisper + Gemma integration
        </li>
        <li>Robot Interface: ROS 2 Jazzy on iRobot Create3
        </li>
      </ul>
  </div>
  </details>
  </div>

  </div>
  </details>
  </div>
  <script>
    const details = document.getElementById('full-report');
    const toggleBtn = document.getElementById('toggle-details');
    if (toggleBtn && details) {
      const setLabel = () => toggleBtn.textContent = details.open ? 'Collapse detailed report' : 'Expand detailed report';
      setLabel();
      toggleBtn.addEventListener('click', () => { details.open = !details.open; setLabel(); });
    }

  if (window.Chart) {
    Chart.defaults.color = '#e5e7eb';
    Chart.defaults.font.family = 'Space Grotesk, -apple-system, sans-serif';
    const latencyCtx = document.getElementById('latencyChart');
    if (latencyCtx) {
      new Chart(latencyCtx, {
        type: 'bar',
        data: {
            labels: ['Porcupine', 'Whisper Tiny', 'Gemma 3 1B', 'Gemini Vision', 'MediaPipe', 'HOG SVM'],
            datasets: [{
              label: 'Latency (ms)',
              data: [10, 3000, 1500, 2500, 50, 30],
              backgroundColor: ['#10b981', '#3b82f6', '#a855f7', '#f59e0b', '#06b6d4', '#f87171']
            }]
          },
          options: {
            scales: {
              y: { beginAtZero: true, grid: { color: 'rgba(255,255,255,0.05)' } },
              x: { grid: { display: false } }
            },
            plugins: {
              legend: { display: false },
              tooltip: { callbacks: { label: ctx => `${ctx.label}: ${ctx.parsed} ms` } }
            }
          }
        });
      }
    }

    // Add language labels to code blocks (Python/Bash/etc.)
    document.querySelectorAll('pre > code[class]').forEach(code => {
      const pre = code.parentElement;
      const cls = code.className || '';
      const langMatch = cls.match(/language-([\\w-]+)/i);
      if (langMatch) {
        pre.setAttribute('data-lang', langMatch[1].toUpperCase());
      }
    });

    // Mark long ASCII diagrams as compact to reduce overflow
    const diagramTitles = [
      'Component Interaction Map',
      'Wake Word Detection Flow',
      'Speech Recognition Flow',
      'Intent Parsing',
      'Vision Description Flow',
      'Human Following System',
      'Detection Hybrid Logic',
      'ROS 2 Control Architecture',
      'Complete Command Flow'
    ];
    document.querySelectorAll('h3').forEach(h3 => {
      if (diagramTitles.some(title => h3.textContent.includes(title))) {
        const next = h3.nextElementSibling;
        if (next && next.tagName.toLowerCase() === 'pre') {
          next.classList.add('diagram');
        }
      }
    });

    // Ensure clicking nav anchors opens the full report section (for Metrics, etc.)
    const fullReport = document.getElementById('full-report');
    document.querySelectorAll('.pill-nav a[href^=\"#\"]').forEach(a => {
      a.addEventListener('click', () => {
        if (fullReport && !fullReport.open) fullReport.open = true;
      });
    });
  </script>
</body>

</html>
